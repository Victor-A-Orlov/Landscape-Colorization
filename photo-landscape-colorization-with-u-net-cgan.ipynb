{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/victororlov/photo-landscape-colorization-with-u-net-cgan?scriptVersionId=89476382\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Table of Contents\n\n* [1. Preparation](#1)\n    * [1.1 Import modules](#1_1)\n    * [1.2 TPU setup](#1_2)\n    * [1.3 Load and preprocess the data](#1_3)\n* [2. Generator](#2)\n    * [2.1 Make a generator](#2_1)\n    * [2.2 Pretrain the generator](#2_2)\n    * [2.3 Standalone generator results](#2_3)\n    * [2.4 Save pretrained generator](#2_4)\n* [3. Discriminator](#3)\n* [4. cGAN](#4)\n    * [4.1 Build the cGAN model](#4_1)\n    * [4.2 Define cGAN loss functions](#4_2)\n    * [4.3 Train the cGAN](#4_3)\n    * [4.4 Final results](#4_4)    \n    * [4.5 Save the final generator](#4_5)\n* [5. References](#another_cell)","metadata":{}},{"cell_type":"markdown","source":"I took the core idea I was implementing here from [this article](https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8) on colorizing the black and white photos: <br>\nThe author is using PyTorch and fast.ai, but i decided to go with Tensorflow. So I found useful the [official Tensorflow tutorial of pix2pix cGAN implementation](http://https://www.tensorflow.org/tutorials/generative/pix2pix?hl=en).<br>\nI started my work by forking the Amy Jang's brilliant [Monet CycleGAN Tutorial](https://www.kaggle.com/amyjang/monet-cyclegan-tutorial)","metadata":{}},{"cell_type":"markdown","source":"# 1. Preparation  <a class=\"anchor\" id=\"1\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Import modules  <a class=\"anchor\" id=\"1_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Due to some compatibility reasons I decided to get the sourse containing `rgb_to_lab` from GitHib instead of importing the `tensorflow_io` module:","metadata":{}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/tensorflow/io/v0.20.0/tensorflow_io/python/experimental/color_ops.py","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:27.792717Z","iopub.execute_input":"2022-02-02T13:28:27.793425Z","iopub.status.idle":"2022-02-02T13:28:28.720912Z","shell.execute_reply.started":"2022-02-02T13:28:27.793294Z","shell.execute_reply":"2022-02-02T13:28:28.719931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Installing the [Segmentation Models](https://github.com/qubvel/segmentation_models) module. It is very useful to construct U-Net from different pretrained backbone models such as ResNet18:","metadata":{}},{"cell_type":"code","source":"!pip install segmentation_models","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:28.723408Z","iopub.execute_input":"2022-02-02T13:28:28.723767Z","iopub.status.idle":"2022-02-02T13:28:39.01146Z","shell.execute_reply.started":"2022-02-02T13:28:28.723725Z","shell.execute_reply":"2022-02-02T13:28:39.010012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:39.013074Z","iopub.execute_input":"2022-02-02T13:28:39.013362Z","iopub.status.idle":"2022-02-02T13:28:39.018122Z","shell.execute_reply.started":"2022-02-02T13:28:39.013328Z","shell.execute_reply":"2022-02-02T13:28:39.017277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport math\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.color import lab2rgb\nimport seaborn as sns\nfrom color_ops import rgb_to_lab","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-02-02T13:28:39.019475Z","iopub.execute_input":"2022-02-02T13:28:39.020322Z","iopub.status.idle":"2022-02-02T13:28:45.697967Z","shell.execute_reply.started":"2022-02-02T13:28:39.020273Z","shell.execute_reply":"2022-02-02T13:28:45.69722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. TPU setup  <a class=\"anchor\" id=\"1_2\"></a>","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:45.699932Z","iopub.execute_input":"2022-02-02T13:28:45.700537Z","iopub.status.idle":"2022-02-02T13:28:51.8465Z","shell.execute_reply.started":"2022-02-02T13:28:45.700503Z","shell.execute_reply":"2022-02-02T13:28:51.845788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3. Load and preprocess the data <a class=\"anchor\" id=\"1_3\"></a>","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path('landscape-pictures')\nprint(GCS_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:51.847676Z","iopub.execute_input":"2022-02-02T13:28:51.847977Z","iopub.status.idle":"2022-02-02T13:28:52.299052Z","shell.execute_reply.started":"2022-02-02T13:28:51.84794Z","shell.execute_reply":"2022-02-02T13:28:52.298327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/*.jpg'))\nprint('Landscape Files:', len(FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:52.300632Z","iopub.execute_input":"2022-02-02T13:28:52.301425Z","iopub.status.idle":"2022-02-02T13:28:52.775925Z","shell.execute_reply.started":"2022-02-02T13:28:52.301385Z","shell.execute_reply":"2022-02-02T13:28:52.775002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COLOR_MODEL = 'lab'\nBATCH_SIZE=1\nIMAGE_SIZE = 256","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:52.777109Z","iopub.execute_input":"2022-02-02T13:28:52.777378Z","iopub.status.idle":"2022-02-02T13:28:52.782185Z","shell.execute_reply.started":"2022-02-02T13:28:52.777344Z","shell.execute_reply":"2022-02-02T13:28:52.781266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned in [[1]](#another_cell), the \"Lab\" color space is the most popular choice for colorization problem. <br>\nThe `L` channel defines perceptual lighness, and `a` and `b` channels define color.\nAs our task is to predict the \"color\" image given the \"colorless\" image, it comes down to predicting the `a` and `b` channels given the `L` channel. <br>\nLets wright a function to load the image in Lab space and normalize it to [-1, 1] range:","metadata":{}},{"cell_type":"code","source":"def load(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n    \n    image = rgb_to_lab(image)\n    lightness = image[:,:,0]\n    lightness = lightness/50-1\n    lightness = lightness[...,tf.newaxis]\n    color = image[:,:,1:]/100\n    return lightness, color","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:52.783392Z","iopub.execute_input":"2022-02-02T13:28:52.783729Z","iopub.status.idle":"2022-02-02T13:28:52.795943Z","shell.execute_reply.started":"2022-02-02T13:28:52.783696Z","shell.execute_reply":"2022-02-02T13:28:52.794986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, let's make a function to get readable grayscale and color images from a dataset instance:","metadata":{}},{"cell_type":"code","source":"def get_image_and_grayscale(input):\n    l, ab = input\n    color_image = np.zeros((IMAGE_SIZE,IMAGE_SIZE,3))\n    color_image[:,:,:1] = l[0,...]*50+50\n    color_image[:,:,1:] = ab[0,...]*100\n    color_image = lab2rgb(color_image)\n    grayscale = np.array(l[0,...,0])\n\n    return color_image, grayscale\n\ndef color_hist(image):\n    for i in range(image.shape[-1]):\n        sns.distplot(image[...,i])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:52.797529Z","iopub.execute_input":"2022-02-02T13:28:52.797855Z","iopub.status.idle":"2022-02-02T13:28:52.807953Z","shell.execute_reply.started":"2022-02-02T13:28:52.797815Z","shell.execute_reply":"2022-02-02T13:28:52.807205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to make a dataset from out filenames, map them with our `load()` function and split into train and validation dataset:","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices(FILENAMES)\ndataset = dataset.map(load, num_parallel_calls=AUTOTUNE)\ndataset_train = dataset.skip(100)\ndataset_val = dataset.take(100)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:52.809001Z","iopub.execute_input":"2022-02-02T13:28:52.809574Z","iopub.status.idle":"2022-02-02T13:28:53.456374Z","shell.execute_reply.started":"2022-02-02T13:28:52.809522Z","shell.execute_reply":"2022-02-02T13:28:53.455515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check what's inside our dataset:","metadata":{}},{"cell_type":"code","source":"for example_input, example_target in dataset.batch(1).take(1):\n    light = example_input[:1,...]\n    color = example_target[:1,...]\n\ndef check_images(l, ab):\n    color_image, grayscale = get_image_and_grayscale((l, ab))\n    plt.figure(figsize=(10,10))\n    plt.subplot(1,2,1)\n    plt.imshow(tf.squeeze(grayscale), cmap='gray')\n    plt.subplot(1,2,2)\n    plt.imshow(tf.squeeze(color_image))\n    \ncheck_images(light, color)\ninput = (light, color)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:53.457841Z","iopub.execute_input":"2022-02-02T13:28:53.458163Z","iopub.status.idle":"2022-02-02T13:28:55.708474Z","shell.execute_reply.started":"2022-02-02T13:28:53.458122Z","shell.execute_reply":"2022-02-02T13:28:55.707859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Generator <a class=\"anchor\" id=\"2\"></a>\n\nAs described in [[1]](#another_cell), in order to avoid the “the blind leading the blind” problem we shall use an already pretrained classification model (in out case the ResNet18) as a downsample path in our U-Net generator. <br>\nThan we should pretrain the generator using just the L1 loss. <br>\nAnd than finally we should train the whole cGAN. <br>\nSo its like we are training the smallest component (in or case its already pretrained), than training the part incorporating it and so on.","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Make a generator  <a class=\"anchor\" id=\"2_1\"></a>\nIn order to make a generator from the ResNet18 in Keras I'll use the [Segmentation Models](https://github.com/qubvel/segmentation_models) library","metadata":{}},{"cell_type":"code","source":"import segmentation_models\nsegmentation_models.set_framework('tf.keras')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:55.709337Z","iopub.execute_input":"2022-02-02T13:28:55.709572Z","iopub.status.idle":"2022-02-02T13:28:55.883821Z","shell.execute_reply.started":"2022-02-02T13:28:55.709527Z","shell.execute_reply":"2022-02-02T13:28:55.882883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_CHANNELS = 2","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:55.88742Z","iopub.execute_input":"2022-02-02T13:28:55.887677Z","iopub.status.idle":"2022-02-02T13:28:55.892252Z","shell.execute_reply.started":"2022-02-02T13:28:55.88765Z","shell.execute_reply":"2022-02-02T13:28:55.891238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Generator():\n    unet = segmentation_models.Unet('resnet18', encoder_weights='imagenet', classes=OUTPUT_CHANNELS, activation='tanh', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), decoder_use_batchnorm=False)\n    inp = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, 1], name='input')\n    x = layers.Concatenate()([inp, inp, inp])\n    x = unet(x)\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    return model\n\nwith strategy.scope():\n    generator = Generator()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:28:55.894342Z","iopub.execute_input":"2022-02-02T13:28:55.894921Z","iopub.status.idle":"2022-02-02T13:29:02.02242Z","shell.execute_reply.started":"2022-02-02T13:28:55.894877Z","shell.execute_reply":"2022-02-02T13:29:02.02158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:29:02.023672Z","iopub.execute_input":"2022-02-02T13:29:02.023986Z","iopub.status.idle":"2022-02-02T13:29:02.953037Z","shell.execute_reply.started":"2022-02-02T13:29:02.023956Z","shell.execute_reply":"2022-02-02T13:29:02.951577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if the generator is working (although it's output will be nonsense):","metadata":{}},{"cell_type":"code","source":"gen_out = generator(example_input, training=False)\ncheck_images(example_input, gen_out)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:29:02.954922Z","iopub.execute_input":"2022-02-02T13:29:02.955268Z","iopub.status.idle":"2022-02-02T13:29:03.872832Z","shell.execute_reply.started":"2022-02-02T13:29:02.955224Z","shell.execute_reply":"2022-02-02T13:29:03.871815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_hist(gen_out)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:29:03.874739Z","iopub.execute_input":"2022-02-02T13:29:03.875041Z","iopub.status.idle":"2022-02-02T13:29:05.127419Z","shell.execute_reply.started":"2022-02-02T13:29:03.874999Z","shell.execute_reply":"2022-02-02T13:29:05.126692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Pretrain the generator <a class=\"anchor\" id=\"2_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now we should compile the model. In the [[2]](#another_cell) they use the L1 objective (Mean Absolute Error), so I'll stick with it for the standalone generator training also.","metadata":{}},{"cell_type":"code","source":"loss_object = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\ngenerator.compile(optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5), loss='mae')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:29:05.12878Z","iopub.execute_input":"2022-02-02T13:29:05.129093Z","iopub.status.idle":"2022-02-02T13:29:05.173422Z","shell.execute_reply.started":"2022-02-02T13:29:05.129052Z","shell.execute_reply":"2022-02-02T13:29:05.172589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator.fit(\n    dataset_train.batch(8, drop_remainder=True),\n    epochs=20,\n    batch_size=8\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:29:05.174897Z","iopub.execute_input":"2022-02-02T13:29:05.17531Z","iopub.status.idle":"2022-02-02T13:31:47.746378Z","shell.execute_reply.started":"2022-02-02T13:29:05.175266Z","shell.execute_reply":"2022-02-02T13:31:47.744716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3. Standalone generator results <a class=\"anchor\" id=\"2_3\"></a>\nLet's look at the intermediate results of the generator pre-training:","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    for example_input, example_target in dataset_val.batch(1).take(5):\n        gen_out = generator(example_input, training=False)\n        check_images(example_input, gen_out)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-02T13:31:47.748074Z","iopub.status.idle":"2022-02-02T13:31:47.748603Z","shell.execute_reply.started":"2022-02-02T13:31:47.748314Z","shell.execute_reply":"2022-02-02T13:31:47.748341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4. Save pretrained generator <a class=\"anchor\" id=\"2_4\"></a>","metadata":{}},{"cell_type":"code","source":"localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\nlocalhost_load_option = tf.saved_model.LoadOptions(experimental_io_device=\"/job:localhost\")\ngenerator.save('generator_lab_deterministic', options=localhost_save_option)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.75003Z","iopub.status.idle":"2022-02-02T13:31:47.75051Z","shell.execute_reply.started":"2022-02-02T13:31:47.750249Z","shell.execute_reply":"2022-02-02T13:31:47.750275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('generator_lab_deterministic', 'zip', './generator_lab_deterministic')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.752082Z","iopub.status.idle":"2022-02-02T13:31:47.752585Z","shell.execute_reply.started":"2022-02-02T13:31:47.752308Z","shell.execute_reply":"2022-02-02T13:31:47.752333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Discriminator <a class=\"anchor\" id=\"3\"></a>\n\nThe discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.","metadata":{}},{"cell_type":"code","source":"def downsample(filters, size, apply_instancenorm=True, name=None):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential(name=name)\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    result.add(layers.LeakyReLU())\n\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.754362Z","iopub.status.idle":"2022-02-02T13:31:47.754879Z","shell.execute_reply.started":"2022-02-02T13:31:47.754605Z","shell.execute_reply":"2022-02-02T13:31:47.754631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False, name=None):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential(name=name)\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.755982Z","iopub.status.idle":"2022-02-02T13:31:47.756339Z","shell.execute_reply.started":"2022-02-02T13:31:47.756161Z","shell.execute_reply":"2022-02-02T13:31:47.756183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 5)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n\n    inp = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, 1], name='input')\n    tar = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, OUTPUT_CHANNELS], name='target')\n#     tar_d = layers.Lambda(color_decoder)(tar)\n#     tar_d = color_decoder(tar)\n    x = layers.Concatenate()([tar, inp])\n#     x = inp\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(IMAGE_SIZE, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.758055Z","iopub.status.idle":"2022-02-02T13:31:47.758394Z","shell.execute_reply.started":"2022-02-02T13:31:47.758225Z","shell.execute_reply":"2022-02-02T13:31:47.758241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras import backend as K \n# K.clear_session()\n\nwith strategy.scope():\n    discriminator = Discriminator()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.759437Z","iopub.status.idle":"2022-02-02T13:31:47.759792Z","shell.execute_reply.started":"2022-02-02T13:31:47.759614Z","shell.execute_reply":"2022-02-02T13:31:47.759631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.760811Z","iopub.status.idle":"2022-02-02T13:31:47.761125Z","shell.execute_reply.started":"2022-02-02T13:31:47.76096Z","shell.execute_reply":"2022-02-02T13:31:47.760976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disc_out = discriminator([example_input, gen_out], training=False)\nplt.imshow(disc_out[0, ..., -1], cmap='RdBu_r')\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.762234Z","iopub.status.idle":"2022-02-02T13:31:47.762621Z","shell.execute_reply.started":"2022-02-02T13:31:47.762427Z","shell.execute_reply":"2022-02-02T13:31:47.762443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. cGAN <a class=\"anchor\" id=\"4\"></a>","metadata":{}},{"cell_type":"markdown","source":"##  4.1. Build the cGAN model <a class=\"anchor\" id=\"4_1\"></a>\n\nWe will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. ","metadata":{}},{"cell_type":"code","source":"class CycleGan(tf.keras.Model):\n    def __init__(\n                self,\n                generator,\n                discriminator,\n                lambda_cycle=10,\n                ):\n        super(CycleGan, self).__init__()\n        self.gen = generator\n        self.disc = discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n                self,\n                gen_optimizer,\n                disc_optimizer,\n                gen_loss_fn,\n                disc_loss_fn,\n                ):\n        super(CycleGan, self).compile()\n        self.gen_optimizer = gen_optimizer\n        self.disc_optimizer = disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        \n    def train_step(self, batch_data):\n        input_image, target = batch_data\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n\n            gen_output = self.gen(input_image, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_output = self.disc([input_image, target], training=True)\n            # discriminator used to check, inputing fake images\n            disc_generated_output = self.disc([input_image, gen_output], training=True)\n\n            # evaluates generator loss\n            gen_total_loss, gen_gan_loss, gen_l1_loss = self.gen_loss_fn(disc_generated_output, gen_output, target)\n            # evaluates discriminator loss\n            disc_loss = self.disc_loss_fn(disc_real_output, disc_generated_output)\n\n        # Calculate the gradients for generator and discriminat\n        generator_gradients = gen_tape.gradient(gen_total_loss,\n                                              self.gen.trainable_variables)\n\n        discriminator_gradients = disc_tape.gradient(disc_loss,\n                                                   self.disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        \n        generator_optimizer.apply_gradients(zip(generator_gradients,\n                                              self.gen.trainable_variables))\n\n        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                                  self.disc.trainable_variables))\n        return {\n                'gen_total_loss': gen_total_loss,\n                'gen_gan_loss': gen_gan_loss,\n                'gen_l1_loss': gen_l1_loss,\n                'disc_loss': disc_loss\n                }","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.763705Z","iopub.status.idle":"2022-02-02T13:31:47.764024Z","shell.execute_reply.started":"2022-02-02T13:31:47.763859Z","shell.execute_reply":"2022-02-02T13:31:47.763874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Define cGAN loss functions <a class=\"anchor\" id=\"4_2\"></a>\n\nThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n    def discriminator_loss(disc_real_output, disc_generated_output):\n        real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n        generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n        total_disc_loss = real_loss + generated_loss\n        return total_disc_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.764891Z","iopub.status.idle":"2022-02-02T13:31:47.765467Z","shell.execute_reply.started":"2022-02-02T13:31:47.765284Z","shell.execute_reply":"2022-02-02T13:31:47.765304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    LAMBDA = 100000\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n    def generator_loss(disc_generated_output, gen_output, target):\n        gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n        # Mean absolute error\n        l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n        total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n        return total_gen_loss, gan_loss, l1_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.766295Z","iopub.status.idle":"2022-02-02T13:31:47.766613Z","shell.execute_reply.started":"2022-02-02T13:31:47.766437Z","shell.execute_reply":"2022-02-02T13:31:47.766453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Train the cGAN <a class=\"anchor\" id=\"4_3\"></a>","metadata":{}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.767731Z","iopub.status.idle":"2022-02-02T13:31:47.768041Z","shell.execute_reply.started":"2022-02-02T13:31:47.767884Z","shell.execute_reply":"2022-02-02T13:31:47.767899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(generator, discriminator)\n    cycle_gan_model.compile(\n        gen_optimizer = generator_optimizer,\n        disc_optimizer = discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.768923Z","iopub.status.idle":"2022-02-02T13:31:47.769227Z","shell.execute_reply.started":"2022-02-02T13:31:47.769069Z","shell.execute_reply":"2022-02-02T13:31:47.769084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cycle_gan_model.fit(\n    dataset.take(len(dataset_train)//8*8).batch(8),\n    epochs=20,\n    batch_size=8,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.770175Z","iopub.status.idle":"2022-02-02T13:31:47.770486Z","shell.execute_reply.started":"2022-02-02T13:31:47.770327Z","shell.execute_reply":"2022-02-02T13:31:47.770343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4. Final results <a class=\"anchor\" id=\"4_4\"></a>","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    for example_input, example_target in dataset_val.batch(1):\n        gen_out = generator(example_input, training=False)\n        check_images(example_input, gen_out)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.771905Z","iopub.status.idle":"2022-02-02T13:31:47.772297Z","shell.execute_reply.started":"2022-02-02T13:31:47.772063Z","shell.execute_reply":"2022-02-02T13:31:47.772077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5. Save the final generator <a class=\"anchor\" id=\"4_5\"></a>","metadata":{}},{"cell_type":"code","source":"localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\ngenerator.save('generator_lab_gan', options=localhost_save_option)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.773143Z","iopub.status.idle":"2022-02-02T13:31:47.773459Z","shell.execute_reply.started":"2022-02-02T13:31:47.773294Z","shell.execute_reply":"2022-02-02T13:31:47.773309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('generator_lab_gan', 'zip', './generator_lab_gan')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:31:47.774664Z","iopub.status.idle":"2022-02-02T13:31:47.774971Z","shell.execute_reply.started":"2022-02-02T13:31:47.774811Z","shell.execute_reply":"2022-02-02T13:31:47.774825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='another_cell'></a>\n# 5. References\n1. Moein Shariatnia (2020). [Colorizing black & white images with U-Net and conditional GAN — A Tutorial](https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8)\n2. Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). [Image-to-image translation with conditional adversarial networks.](https://arxiv.org/abs/1611.07004) In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134).\n","metadata":{}}]}